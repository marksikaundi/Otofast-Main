{
  "title": "Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows",
  "publishedAt": "2023-10-17T00:00:00.000Z",
  "updatedAt": "2023-10-17T00:00:00.000Z",
  "description": "TensorRT also now accelerating Stable Diffusion, plus RTX Video Super Resolution update releases.",
  "image": {
    "filePath": "../public/blogs/Large Language Models up to 4x Faster on RTX .png",
    "relativeFilePath": "../../public/blogs/Large Language Models up to 4x Faster on RTX .png",
    "format": "png",
    "height": 718,
    "width": 2048,
    "aspectRatio": 2.852367688022284,
    "blurhashDataUrl": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAMAAADz0U65AAAAaVBMVEWAX1OzuLOoaEmxtbGBcGd/TDavn5O2sqh6d3aKWETAeVFxbHF2U0Oxb02JaWScYEWrr6mkgHJvYGBQNCtVPTWXgGuNhHqiin7UhFWBlDdAPEJzTjt4amOEdG1CQUV4aW5URUN1QSuhl4g5j9shAAAACXBIWXMAAAsTAAALEwEAmpwYAAAARElEQVR4nAXBhQGAMAADsM6Yu+D6/5EkIIyRNPSHJJNU7R2YppxLEx0zdBRVdARNTawPB7xZ9uvmcNS7rQTgOLlVyq4/eKADezklOIUAAAAASUVORK5CYII="
  },
  "isPublished": true,
  "author": "Mark Sikaundi",
  "tags": [
    "generative-ai"
  ],
  "body": {
    "raw": "\nGeForce RTX and NVIDIA RTX GPUs\n\nGenerative AI is one of the most important trends in the history of personal computing, bringing advancements to gaming, creativity, video, productivity, development and more.\n\nAnd GeForce RTX and NVIDIA RTX GPUs, which are packed with dedicated AI processors called Tensor Cores, are bringing the power of generative AI natively to more than 100 million Windows PCs and workstations.\n\nToday, generative AI on PC is getting up to 4x faster via TensorRT-LLM for Windows, an open-source library that accelerates inference performance for the latest AI large language models, like Llama 2 and Code Llama. This follows the announcement of TensorRT-LLM for data centers last month.\n\n### NVIDIA tools\n\nNVIDIA has also released tools to help developers accelerate their LLMs, including scripts that optimize custom models with TensorRT-LLM, TensorRT-optimized open-source models and a developer reference project that showcases both the speed and quality of LLM responses.\n\nTensorRT acceleration is now available for Stable Diffusion in the popular Web UI by Automatic1111 distribution. It speeds up the generative AI diffusion model by up to 2x over the previous fastest implementation.\n\nPlus, RTX Video Super Resolution (VSR) version 1.5 is available as part of today’s Game Ready Driver release — and will be available in the next NVIDIA Studio Driver, releasing early next month.\n\n### Supercharging LLMs With TensorRT\n\nLLMs are fueling productivity — engaging in chat, summarizing documents and web content, drafting emails and blogs — and are at the core of new pipelines of AI and other software that can automatically analyze data and generate a vast array of content.\n\n### TensorRT-LLM\n\nTensorRT-LLM, a library for accelerating LLM inference, gives developers and end users the benefit of LLMs that can now operate up to 4x faster on RTX-powered Windows PCs.\n\nAt higher batch sizes, this acceleration significantly improves the experience for more sophisticated LLM use — like writing and coding assistants that output multiple, unique auto-complete results at once. The result is accelerated performance and improved quality that lets users select the best of the bunch.\n\n### TensorRT-LLM\n\nTensorRT-LLM acceleration is also beneficial when integrating LLM capabilities with other technology, such as in retrieval-augmented generation (RAG), where an LLM is paired with a vector library or vector database. RAG enables the LLM to deliver responses based on a specific dataset, like user emails or articles on a website, to provide more targeted answers.\n\n### How does NVIDIA ACE\n\nTo show this in practical terms, when the question “How does NVIDIA ACE generate emotional responses?” was asked of the LLaMa 2 base model, it returned an unhelpful response.\n\nConversely, using RAG with recent GeForce news articles loaded into a vector library and connected to the same Llama 2 model not only returned the correct answer — using NeMo SteerLM — but did so much quicker with TensorRT-LLM acceleration. This combination of speed and proficiency gives users smarter solutions.\n\nTensorRT-LLM will soon be available to download from the NVIDIA Developer website. TensorRT-optimized open source models and the RAG demo with GeForce news as a sample project are available at ngc.nvidia.com and GitHub.com/NVIDIA.\n\n### Automatic Acceleration\n\nDiffusion models, like Stable Diffusion, are used to imagine and create stunning, novel works of art. Image generation is an iterative process that can take hundreds of cycles to achieve the perfect output. When done on an underpowered computer, this iteration can add up to hours of wait time.\n\n### TensorRT\n\nTensorRT is designed to accelerate AI models through layer fusion, precision calibration, kernel auto-tuning and other capabilities that significantly boost inference efficiency and speed. This makes it indispensable for real-time applications and resource-intensive tasks.\n\nAnd now, TensorRT doubles the speed of Stable Diffusion.\n\n### WebUI\n\nCompatible with the most popular distribution, WebUI from Automatic1111, Stable Diffusion with TensorRT acceleration helps users iterate faster and spend less time waiting on the computer, delivering a final image sooner. On a GeForce RTX 4090, it runs 7x faster than the top implementation on Macs with an Apple M2 Ultra. The extension is available for download today.\n\nThe TensorRT demo of a Stable Diffusion pipeline provides developers with a reference implementation on how to prepare diffusion models and accelerate them using TensorRT. This is the starting point for developers interested in turbocharging a diffusion pipeline and bringing lightning-fast inferencing to applications.\n\n### Video That’s Super\n\nAI is improving everyday PC experiences for all users. Streaming video — from nearly any source, like YouTube, Twitch, Prime Video, Disney+ and countless others — is among the most popular activities on a PC. Thanks to AI and RTX, it’s getting another update in image quality.\n\nRTX VSR is a breakthrough in AI pixel processing that improves the quality of streamed video content by reducing or eliminating artifacts caused by video compression. It also sharpens edges and details.\n\n### Video sample\n\nAvailable now, RTX VSR version 1.5 further improves visual quality with updated models, de-artifacts content played in its native resolution and adds support for RTX GPUs based on the NVIDIA Turing architecture — both professional RTX and GeForce RTX 20 Series GPUs.\n\nRetraining the VSR AI model helped it learn to accurately identify the difference between subtle details and compression artifacts. As a result, AI-enhanced images more accurately preserve details during the upscaling process. Finer details are more visible, and the overall image looks sharper and crisper.\n\nNew with version 1.5 is the ability to de-artifact video played at the display’s native resolution. The original release only enhanced video when it was being upscaled. Now, for example, 1080p video streamed to a 1080p resolution display will look smoother as heavy artifacts are reduced.\n\nRTX VSR 1.5 is available today for all RTX users in the latest Game Ready Driver. It will be available in the upcoming NVIDIA Studio Driver, scheduled for early next month.\n\nRTX VSR is among the NVIDIA software, tools, libraries and SDKs — like those mentioned above, plus DLSS, Omniverse, AI Workbench and others — that have helped bring over 400 AI-enabled apps and games to consumers.\n\nThe AI era is upon us. And RTX is supercharging at every step in its evolution.\n",
    "code": "var Component=(()=>{var dn=Object.create;var P=Object.defineProperty;var ln=Object.getOwnPropertyDescriptor;var cn=Object.getOwnPropertyNames;var fn=Object.getPrototypeOf,mn=Object.prototype.hasOwnProperty;var q=(l,r)=>()=>(r||l((r={exports:{}}).exports,r),r.exports),bn=(l,r)=>{for(var h in r)P(l,h,{get:r[h],enumerable:!0})},ye=(l,r,h,g)=>{if(r&&typeof r==\"object\"||typeof r==\"function\")for(let x of cn(r))!mn.call(l,x)&&x!==h&&P(l,x,{get:()=>r[x],enumerable:!(g=ln(r,x))||g.enumerable});return l};var pn=(l,r,h)=>(h=l!=null?dn(fn(l)):{},ye(r||!l||!l.__esModule?P(h,\"default\",{value:l,enumerable:!0}):h,l)),hn=l=>ye(P({},\"__esModule\",{value:!0}),l);var Ne=q((kn,ke)=>{ke.exports=React});var Te=q(z=>{\"use strict\";(function(){\"use strict\";var l=Ne(),r=Symbol.for(\"react.element\"),h=Symbol.for(\"react.portal\"),g=Symbol.for(\"react.fragment\"),x=Symbol.for(\"react.strict_mode\"),B=Symbol.for(\"react.profiler\"),H=Symbol.for(\"react.provider\"),K=Symbol.for(\"react.context\"),w=Symbol.for(\"react.forward_ref\"),L=Symbol.for(\"react.suspense\"),O=Symbol.for(\"react.suspense_list\"),E=Symbol.for(\"react.memo\"),j=Symbol.for(\"react.lazy\"),Ee=Symbol.for(\"react.offscreen\"),J=Symbol.iterator,Ue=\"@@iterator\";function Ie(e){if(e===null||typeof e!=\"object\")return null;var n=J&&e[J]||e[Ue];return typeof n==\"function\"?n:null}var k=l.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function m(e){{for(var n=arguments.length,i=new Array(n>1?n-1:0),a=1;a<n;a++)i[a-1]=arguments[a];Se(\"error\",e,i)}}function Se(e,n,i){{var a=k.ReactDebugCurrentFrame,u=a.getStackAddendum();u!==\"\"&&(n+=\"%s\",i=i.concat([u]));var d=i.map(function(o){return String(o)});d.unshift(\"Warning: \"+n),Function.prototype.apply.call(console[e],console,d)}}var Ae=!1,Ce=!1,Pe=!1,Le=!1,Oe=!1,Z;Z=Symbol.for(\"react.module.reference\");function je(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===g||e===B||Oe||e===x||e===L||e===O||Le||e===Ee||Ae||Ce||Pe||typeof e==\"object\"&&e!==null&&(e.$$typeof===j||e.$$typeof===E||e.$$typeof===H||e.$$typeof===K||e.$$typeof===w||e.$$typeof===Z||e.getModuleId!==void 0))}function Me(e,n,i){var a=e.displayName;if(a)return a;var u=n.displayName||n.name||\"\";return u!==\"\"?i+\"(\"+u+\")\":i}function Q(e){return e.displayName||\"Context\"}function _(e){if(e==null)return null;if(typeof e.tag==\"number\"&&m(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case g:return\"Fragment\";case h:return\"Portal\";case B:return\"Profiler\";case x:return\"StrictMode\";case L:return\"Suspense\";case O:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case K:var n=e;return Q(n)+\".Consumer\";case H:var i=e;return Q(i._context)+\".Provider\";case w:return Me(e,e.render,\"ForwardRef\");case E:var a=e.displayName||null;return a!==null?a:_(e.type)||\"Memo\";case j:{var u=e,d=u._payload,o=u._init;try{return _(o(d))}catch{return null}}}return null}var y=Object.assign,R=0,ee,ne,re,ie,te,ae,se;function oe(){}oe.__reactDisabledLog=!0;function Fe(){{if(R===0){ee=console.log,ne=console.info,re=console.warn,ie=console.error,te=console.group,ae=console.groupCollapsed,se=console.groupEnd;var e={configurable:!0,enumerable:!0,value:oe,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}R++}}function Ve(){{if(R--,R===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:y({},e,{value:ee}),info:y({},e,{value:ne}),warn:y({},e,{value:re}),error:y({},e,{value:ie}),group:y({},e,{value:te}),groupCollapsed:y({},e,{value:ae}),groupEnd:y({},e,{value:se})})}R<0&&m(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var M=k.ReactCurrentDispatcher,F;function U(e,n,i){{if(F===void 0)try{throw Error()}catch(u){var a=u.stack.trim().match(/\\n( *(at )?)/);F=a&&a[1]||\"\"}return`\n`+F+e}}var V=!1,I;{var We=typeof WeakMap==\"function\"?WeakMap:Map;I=new We}function ue(e,n){if(!e||V)return\"\";{var i=I.get(e);if(i!==void 0)return i}var a;V=!0;var u=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var d;d=M.current,M.current=null,Fe();try{if(n){var o=function(){throw Error()};if(Object.defineProperty(o.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(o,[])}catch(v){a=v}Reflect.construct(e,[],o)}else{try{o.call()}catch(v){a=v}e.call(o.prototype)}}else{try{throw Error()}catch(v){a=v}e()}}catch(v){if(v&&a&&typeof v.stack==\"string\"){for(var s=v.stack.split(`\n`),b=a.stack.split(`\n`),c=s.length-1,f=b.length-1;c>=1&&f>=0&&s[c]!==b[f];)f--;for(;c>=1&&f>=0;c--,f--)if(s[c]!==b[f]){if(c!==1||f!==1)do if(c--,f--,f<0||s[c]!==b[f]){var p=`\n`+s[c].replace(\" at new \",\" at \");return e.displayName&&p.includes(\"<anonymous>\")&&(p=p.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&I.set(e,p),p}while(c>=1&&f>=0);break}}}finally{V=!1,M.current=d,Ve(),Error.prepareStackTrace=u}var T=e?e.displayName||e.name:\"\",ge=T?U(T):\"\";return typeof e==\"function\"&&I.set(e,ge),ge}function Xe(e,n,i){return ue(e,!1)}function Ge(e){var n=e.prototype;return!!(n&&n.isReactComponent)}function S(e,n,i){if(e==null)return\"\";if(typeof e==\"function\")return ue(e,Ge(e));if(typeof e==\"string\")return U(e);switch(e){case L:return U(\"Suspense\");case O:return U(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case w:return Xe(e.render);case E:return S(e.type,n,i);case j:{var a=e,u=a._payload,d=a._init;try{return S(d(u),n,i)}catch{}}}return\"\"}var A=Object.prototype.hasOwnProperty,de={},le=k.ReactDebugCurrentFrame;function C(e){if(e){var n=e._owner,i=S(e.type,e._source,n?n.type:null);le.setExtraStackFrame(i)}else le.setExtraStackFrame(null)}function Ye(e,n,i,a,u){{var d=Function.call.bind(A);for(var o in e)if(d(e,o)){var s=void 0;try{if(typeof e[o]!=\"function\"){var b=Error((a||\"React class\")+\": \"+i+\" type `\"+o+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[o]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw b.name=\"Invariant Violation\",b}s=e[o](n,o,a,i,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(c){s=c}s&&!(s instanceof Error)&&(C(u),m(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",a||\"React class\",i,o,typeof s),C(null)),s instanceof Error&&!(s.message in de)&&(de[s.message]=!0,C(u),m(\"Failed %s type: %s\",i,s.message),C(null))}}}var $e=Array.isArray;function W(e){return $e(e)}function qe(e){{var n=typeof Symbol==\"function\"&&Symbol.toStringTag,i=n&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return i}}function ze(e){try{return ce(e),!1}catch{return!0}}function ce(e){return\"\"+e}function fe(e){if(ze(e))return m(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",qe(e)),ce(e)}var D=k.ReactCurrentOwner,Be={key:!0,ref:!0,__self:!0,__source:!0},me,be,X;X={};function He(e){if(A.call(e,\"ref\")){var n=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(n&&n.isReactWarning)return!1}return e.ref!==void 0}function Ke(e){if(A.call(e,\"key\")){var n=Object.getOwnPropertyDescriptor(e,\"key\").get;if(n&&n.isReactWarning)return!1}return e.key!==void 0}function Je(e,n){if(typeof e.ref==\"string\"&&D.current&&n&&D.current.stateNode!==n){var i=_(D.current.type);X[i]||(m('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',_(D.current.type),e.ref),X[i]=!0)}}function Ze(e,n){{var i=function(){me||(me=!0,m(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",n))};i.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:i,configurable:!0})}}function Qe(e,n){{var i=function(){be||(be=!0,m(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",n))};i.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:i,configurable:!0})}}var en=function(e,n,i,a,u,d,o){var s={$$typeof:r,type:e,key:n,ref:i,props:o,_owner:d};return s._store={},Object.defineProperty(s._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(s,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:a}),Object.defineProperty(s,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:u}),Object.freeze&&(Object.freeze(s.props),Object.freeze(s)),s};function nn(e,n,i,a,u){{var d,o={},s=null,b=null;i!==void 0&&(fe(i),s=\"\"+i),Ke(n)&&(fe(n.key),s=\"\"+n.key),He(n)&&(b=n.ref,Je(n,u));for(d in n)A.call(n,d)&&!Be.hasOwnProperty(d)&&(o[d]=n[d]);if(e&&e.defaultProps){var c=e.defaultProps;for(d in c)o[d]===void 0&&(o[d]=c[d])}if(s||b){var f=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;s&&Ze(o,f),b&&Qe(o,f)}return en(e,s,b,u,a,D.current,o)}}var G=k.ReactCurrentOwner,pe=k.ReactDebugCurrentFrame;function N(e){if(e){var n=e._owner,i=S(e.type,e._source,n?n.type:null);pe.setExtraStackFrame(i)}else pe.setExtraStackFrame(null)}var Y;Y=!1;function $(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===r}function he(){{if(G.current){var e=_(G.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function rn(e){{if(e!==void 0){var n=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),i=e.lineNumber;return`\n\nCheck your code at `+n+\":\"+i+\".\"}return\"\"}}var _e={};function tn(e){{var n=he();if(!n){var i=typeof e==\"string\"?e:e.displayName||e.name;i&&(n=`\n\nCheck the top-level render call using <`+i+\">.\")}return n}}function ve(e,n){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var i=tn(n);if(_e[i])return;_e[i]=!0;var a=\"\";e&&e._owner&&e._owner!==G.current&&(a=\" It was passed a child from \"+_(e._owner.type)+\".\"),N(e),m('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',i,a),N(null)}}function xe(e,n){{if(typeof e!=\"object\")return;if(W(e))for(var i=0;i<e.length;i++){var a=e[i];$(a)&&ve(a,n)}else if($(e))e._store&&(e._store.validated=!0);else if(e){var u=Ie(e);if(typeof u==\"function\"&&u!==e.entries)for(var d=u.call(e),o;!(o=d.next()).done;)$(o.value)&&ve(o.value,n)}}}function an(e){{var n=e.type;if(n==null||typeof n==\"string\")return;var i;if(typeof n==\"function\")i=n.propTypes;else if(typeof n==\"object\"&&(n.$$typeof===w||n.$$typeof===E))i=n.propTypes;else return;if(i){var a=_(n);Ye(i,e.props,\"prop\",a,e)}else if(n.PropTypes!==void 0&&!Y){Y=!0;var u=_(n);m(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",u||\"Unknown\")}typeof n.getDefaultProps==\"function\"&&!n.getDefaultProps.isReactClassApproved&&m(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function sn(e){{for(var n=Object.keys(e.props),i=0;i<n.length;i++){var a=n[i];if(a!==\"children\"&&a!==\"key\"){N(e),m(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",a),N(null);break}}e.ref!==null&&(N(e),m(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),N(null))}}function on(e,n,i,a,u,d){{var o=je(e);if(!o){var s=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(s+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var b=rn(u);b?s+=b:s+=he();var c;e===null?c=\"null\":W(e)?c=\"array\":e!==void 0&&e.$$typeof===r?(c=\"<\"+(_(e.type)||\"Unknown\")+\" />\",s=\" Did you accidentally export a JSX literal instead of a component?\"):c=typeof e,m(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",c,s)}var f=nn(e,n,i,u,d);if(f==null)return f;if(o){var p=n.children;if(p!==void 0)if(a)if(W(p)){for(var T=0;T<p.length;T++)xe(p[T],e);Object.freeze&&Object.freeze(p)}else m(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else xe(p,e)}return e===g?sn(f):an(f),f}}var un=on;z.Fragment=g,z.jsxDEV=un})()});var De=q((Tn,Re)=>{\"use strict\";Re.exports=Te()});var gn={};bn(gn,{default:()=>xn,frontmatter:()=>_n});var t=pn(De()),_n={title:\"Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows\",description:\"TensorRT also now accelerating Stable Diffusion, plus RTX Video Super Resolution update releases.\",image:\"../../public/blogs/Large Language Models up to 4x Faster on RTX .png\",publishedAt:\"2023-10-17\",updatedAt:\"2023-10-17\",author:\"Mark Sikaundi\",isPublished:!0,tags:[\"generative-ai\"]};function we(l){let r=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\"},l.components);return(0,t.jsxDEV)(t.Fragment,{children:[(0,t.jsxDEV)(r.p,{children:\"GeForce RTX and NVIDIA RTX GPUs\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:13,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Generative AI is one of the most important trends in the history of personal computing, bringing advancements to gaming, creativity, video, productivity, development and more.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:15,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"And GeForce RTX and NVIDIA RTX GPUs, which are packed with dedicated AI processors called Tensor Cores, are bringing the power of generative AI natively to more than 100 million Windows PCs and workstations.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:17,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Today, generative AI on PC is getting up to 4x faster via TensorRT-LLM for Windows, an open-source library that accelerates inference performance for the latest AI large language models, like Llama 2 and Code Llama. This follows the announcement of TensorRT-LLM for data centers last month.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:19,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"nvidia-tools\",children:[\"NVIDIA tools\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#nvidia-tools\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:21,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"NVIDIA has also released tools to help developers accelerate their LLMs, including scripts that optimize custom models with TensorRT-LLM, TensorRT-optimized open-source models and a developer reference project that showcases both the speed and quality of LLM responses.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:23,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"TensorRT acceleration is now available for Stable Diffusion in the popular Web UI by Automatic1111 distribution. It speeds up the generative AI diffusion model by up to 2x over the previous fastest implementation.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:25,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Plus, RTX Video Super Resolution (VSR) version 1.5 is available as part of today\\u2019s Game Ready Driver release \\u2014 and will be available in the next NVIDIA Studio Driver, releasing early next month.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:27,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"supercharging-llms-with-tensorrt\",children:[\"Supercharging LLMs With TensorRT\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#supercharging-llms-with-tensorrt\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:29,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"LLMs are fueling productivity \\u2014 engaging in chat, summarizing documents and web content, drafting emails and blogs \\u2014 and are at the core of new pipelines of AI and other software that can automatically analyze data and generate a vast array of content.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:31,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"tensorrt-llm\",children:[\"TensorRT-LLM\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#tensorrt-llm\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:33,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"TensorRT-LLM, a library for accelerating LLM inference, gives developers and end users the benefit of LLMs that can now operate up to 4x faster on RTX-powered Windows PCs.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:35,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"At higher batch sizes, this acceleration significantly improves the experience for more sophisticated LLM use \\u2014 like writing and coding assistants that output multiple, unique auto-complete results at once. The result is accelerated performance and improved quality that lets users select the best of the bunch.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:37,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"tensorrt-llm-1\",children:[\"TensorRT-LLM\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#tensorrt-llm-1\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:39,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"TensorRT-LLM acceleration is also beneficial when integrating LLM capabilities with other technology, such as in retrieval-augmented generation (RAG), where an LLM is paired with a vector library or vector database. RAG enables the LLM to deliver responses based on a specific dataset, like user emails or articles on a website, to provide more targeted answers.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:41,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"how-does-nvidia-ace\",children:[\"How does NVIDIA ACE\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#how-does-nvidia-ace\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:43,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"To show this in practical terms, when the question \\u201CHow does NVIDIA ACE generate emotional responses?\\u201D was asked of the LLaMa 2 base model, it returned an unhelpful response.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:45,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Conversely, using RAG with recent GeForce news articles loaded into a vector library and connected to the same Llama 2 model not only returned the correct answer \\u2014 using NeMo SteerLM \\u2014 but did so much quicker with TensorRT-LLM acceleration. This combination of speed and proficiency gives users smarter solutions.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:47,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"TensorRT-LLM will soon be available to download from the NVIDIA Developer website. TensorRT-optimized open source models and the RAG demo with GeForce news as a sample project are available at ngc.nvidia.com and GitHub.com/NVIDIA.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:49,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"automatic-acceleration\",children:[\"Automatic Acceleration\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#automatic-acceleration\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:51,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Diffusion models, like Stable Diffusion, are used to imagine and create stunning, novel works of art. Image generation is an iterative process that can take hundreds of cycles to achieve the perfect output. When done on an underpowered computer, this iteration can add up to hours of wait time.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:53,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"tensorrt\",children:[\"TensorRT\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#tensorrt\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:55,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"TensorRT is designed to accelerate AI models through layer fusion, precision calibration, kernel auto-tuning and other capabilities that significantly boost inference efficiency and speed. This makes it indispensable for real-time applications and resource-intensive tasks.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:57,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"And now, TensorRT doubles the speed of Stable Diffusion.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:59,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"webui\",children:[\"WebUI\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#webui\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:61,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Compatible with the most popular distribution, WebUI from Automatic1111, Stable Diffusion with TensorRT acceleration helps users iterate faster and spend less time waiting on the computer, delivering a final image sooner. On a GeForce RTX 4090, it runs 7x faster than the top implementation on Macs with an Apple M2 Ultra. The extension is available for download today.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:63,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"The TensorRT demo of a Stable Diffusion pipeline provides developers with a reference implementation on how to prepare diffusion models and accelerate them using TensorRT. This is the starting point for developers interested in turbocharging a diffusion pipeline and bringing lightning-fast inferencing to applications.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:65,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"video-thats-super\",children:[\"Video That\\u2019s Super\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#video-thats-super\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:67,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"AI is improving everyday PC experiences for all users. Streaming video \\u2014 from nearly any source, like YouTube, Twitch, Prime Video, Disney+ and countless others \\u2014 is among the most popular activities on a PC. Thanks to AI and RTX, it\\u2019s getting another update in image quality.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:69,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"RTX VSR is a breakthrough in AI pixel processing that improves the quality of streamed video content by reducing or eliminating artifacts caused by video compression. It also sharpens edges and details.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:71,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.h3,{id:\"video-sample\",children:[\"Video sample\",(0,t.jsxDEV)(r.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#video-sample\",children:(0,t.jsxDEV)(r.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:73,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Available now, RTX VSR version 1.5 further improves visual quality with updated models, de-artifacts content played in its native resolution and adds support for RTX GPUs based on the NVIDIA Turing architecture \\u2014 both professional RTX and GeForce RTX 20 Series GPUs.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:75,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"Retraining the VSR AI model helped it learn to accurately identify the difference between subtle details and compression artifacts. As a result, AI-enhanced images more accurately preserve details during the upscaling process. Finer details are more visible, and the overall image looks sharper and crisper.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:77,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"New with version 1.5 is the ability to de-artifact video played at the display\\u2019s native resolution. The original release only enhanced video when it was being upscaled. Now, for example, 1080p video streamed to a 1080p resolution display will look smoother as heavy artifacts are reduced.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:79,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"RTX VSR 1.5 is available today for all RTX users in the latest Game Ready Driver. It will be available in the upcoming NVIDIA Studio Driver, scheduled for early next month.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:81,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"RTX VSR is among the NVIDIA software, tools, libraries and SDKs \\u2014 like those mentioned above, plus DLSS, Omniverse, AI Workbench and others \\u2014 that have helped bring over 400 AI-enabled apps and games to consumers.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:83,columnNumber:1},this),`\n`,(0,t.jsxDEV)(r.p,{children:\"The AI era is upon us. And RTX is supercharging at every step in its evolution.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:85,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\",lineNumber:1,columnNumber:1},this)}function vn(l={}){let{wrapper:r}=l.components||{};return r?(0,t.jsxDEV)(r,Object.assign({},l,{children:(0,t.jsxDEV)(we,l,void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this)}),void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/explore-ui/content/_mdx_bundler_entry_point-5a0ed4ec-70af-4b57-9e77-7cd47722e720.mdx\"},this):we(l)}var xn=vn;return hn(gn);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "large-language-models-up-to-4x-faster-on-rtx/index.mdx",
  "_raw": {
    "sourceFilePath": "large-language-models-up-to-4x-faster-on-rtx/index.mdx",
    "sourceFileName": "index.mdx",
    "sourceFileDir": "large-language-models-up-to-4x-faster-on-rtx",
    "contentType": "mdx",
    "flattenedPath": "large-language-models-up-to-4x-faster-on-rtx"
  },
  "type": "Blog",
  "url": "/blogs/large-language-models-up-to-4x-faster-on-rtx",
  "readingTime": {
    "text": "6 min read",
    "minutes": 5.09,
    "time": 305400,
    "words": 1018
  },
  "toc": [
    {
      "level": "three",
      "text": "NVIDIA tools",
      "slug": "nvidia-tools"
    },
    {
      "level": "three",
      "text": "Supercharging LLMs With TensorRT",
      "slug": "supercharging-llms-with-tensorrt"
    },
    {
      "level": "three",
      "text": "TensorRT-LLM",
      "slug": "tensorrt-llm"
    },
    {
      "level": "three",
      "text": "TensorRT-LLM",
      "slug": "tensorrt-llm-1"
    },
    {
      "level": "three",
      "text": "How does NVIDIA ACE",
      "slug": "how-does-nvidia-ace"
    },
    {
      "level": "three",
      "text": "Automatic Acceleration",
      "slug": "automatic-acceleration"
    },
    {
      "level": "three",
      "text": "TensorRT",
      "slug": "tensorrt"
    },
    {
      "level": "three",
      "text": "WebUI",
      "slug": "webui"
    },
    {
      "level": "three",
      "text": "Video That’s Super",
      "slug": "video-thats-super"
    },
    {
      "level": "three",
      "text": "Video sample",
      "slug": "video-sample"
    }
  ]
}